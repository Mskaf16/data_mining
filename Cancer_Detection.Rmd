---
title: "Cancer Detection"
author: "Michelle Skaf"
date: "8/23/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE) #global setting in knitting to show code all through the knitting 
setwd("~/Documents/MSBA2022/DataMining")
```


```{r message=FALSE,  warning=FALSE}
# load the required libraries
library("readxl") # used to read excel files
library("dplyr") # used for data munging 
library("FNN") # used for knn regression (knn.reg function)
library("caret") # used for various predictive models
library("class") # for using confusion matrix function
library("rpart.plot") # used to plot decision tree
library("rpart")  # used for Regression tree
library("glmnet") # used for Lasso and Ridge regression
library('NeuralNetTools') # used to plot Neural Networks
library("PRROC") # top plot ROC curve
library("ROCR") # top plot lift curve
library("e1071")
library("skimr")
```


## 1.1 Data loading and transformation

```{r }
# Load the Breast Cancer data set

cancer_data = read.csv("wdbc.data", header = FALSE)
cancer_data$V2 <- as.factor(cancer_data$V2)

# create Y and X data frames
cancer_y = cancer_data %>% pull("V2")
# exclude V1 since its a row number
cancer_x = cancer_data %>% select(-c("V1", "V2"))
```

Create a function that normalises columns since scale for each column might be different.

```{r }
# function to normalize data (0 to 1)
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```

```{r }
# Normalize x variables since they are at different scale
cancer_x_normalized <- as.data.frame(lapply(cancer_x, normalize))
```

Create Training and Testing data sets

```{r }
# 75% of the data is used for training and rest for testing
smp_size <- floor(0.75 * nrow(cancer_x_normalized))

# randomly select row numbers for training data set
#seq_len will take one argument and create a sequence from 1 to that argument
train_ind <- sample(seq_len(nrow(cancer_x_normalized)), size = smp_size)

# creating test and training sets for x
cancer_x_train <- cancer_x_normalized[train_ind, ]
cancer_x_test <- cancer_x_normalized[-train_ind, ]

# creating test and training sets for y
cancer_y_train <- cancer_y[train_ind]
cancer_y_test <- cancer_y[-train_ind]

# Create an empty data frame to store results from different models
clf_results <- data.frame(matrix(ncol = 5, nrow = 0))
names(clf_results) <- c("Model", "Accuracy", "Precision", "Recall", "F1")

```


## 1.2 KNN Classification

```{r }

# Cross validation 
cross_validation <- trainControl(## 10-fold CV
                                method = "repeatedcv",
                                number = 10,
                                ## repeated three times
                                repeats = 3)


# Hyperparamter tuning
# k = number of nrearest neighbours
Param_Grid <-  expand.grid( k = 1:10)

# fit the model to training data
knn_clf_fit <- train(cancer_x_train,
                     cancer_y_train, 
                     method = "knn",
                     tuneGrid = Param_Grid,
                     trControl = cross_validation )

# check the accuracy for different models
knn_clf_fit

```

```{r }
# Plot accuracies for different k values
plot(knn_clf_fit)

# print the best model
print(knn_clf_fit$finalModel)
```

```{r }
# Predict on test data
knnPredict <- predict(knn_clf_fit, newdata = cancer_x_test) 

```


```{r }
# Print Confusion matrix, Accuracy, Sensitivity etc 
confusionMatrix(knnPredict, cancer_y_test)

# Add results into clf_results dataframe
x1 <- confusionMatrix(knnPredict, cancer_y_test)[["overall"]]
y1 <- confusionMatrix(knnPredict, cancer_y_test)[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "KNN", 
                                             Accuracy = round (x1[["Accuracy"]],3), 
                                            Precision = round (y1[["Precision"]],3), 
                                            Recall = round (y1[["Recall"]],3), 
                                            F1 = round (y1[["F1"]],3))
# Print Accuracy and F1 score

cat("Accuarcy is ", round(x1[["Accuracy"]],3), "and F1 is ", round (y1[["F1"]],3)  )


```


## 1.3 Decision Tree Classification 

```{r }

# Cross validation
cross_validation <- trainControl(## 10-fold CV
                                method = "repeatedcv",
                                number = 10,
                                ## repeated three times
                                repeats = 3)
# Hyperparamter tuning
# maxdepth =  the maximum depth of the tree that will be created or
# the length of the longest path from the tree root to a leaf.

Param_Grid <-  expand.grid(maxdepth = 2:10)

dtree_fit <- train(cancer_x_train,
                   cancer_y_train, 
                   method = "rpart2",
                   # split - criteria to split nodes
                   parms = list(split = "gini"),
                  tuneGrid = Param_Grid,
                   trControl = cross_validation,
                  # preProc -  perform listed pre-processing to predictor dataframe
                   preProc = c("center", "scale"))

# check the accuracy for different models
dtree_fit
```


```{r }
# print the final model
dtree_fit$finalModel
```

```{r }
# Plot decision tree
prp(dtree_fit$finalModel, box.palette = "Reds", tweak = 1.2)
```


```{r }
# Predict on test data
dtree_predict <- predict(dtree_fit, newdata = cancer_x_test)
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(dtree_predict,  cancer_y_test )

# Add results into clf_results dataframe
x2 <- confusionMatrix(dtree_predict,  cancer_y_test )[["overall"]]
y2 <- confusionMatrix(dtree_predict,  cancer_y_test )[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Decision Tree", 
                                             Accuracy = round (x2[["Accuracy"]],3), 
                                            Precision = round (y2[["Precision"]],3), 
                                            Recall = round (y2[["Recall"]],3), 
                                            F1 = round (y2[["F1"]],3))

# Print Accuracy and F1 score

cat("Accuarcy is ", round(x2[["Accuracy"]],3), "and F1 is ", round (y2[["F1"]],3)  )



```

## 1.4 Logistic regression

Convert categorical outcome into numerical. 

```{r }
cancer_y_train_l <- ifelse(cancer_y_train =="B", 1, 0)
cancer_y_test_l <- ifelse(cancer_y_test =="B", 1, 0)
```

```{r  message=FALSE,  warning=FALSE}
glm_fit <- train(cancer_x_train,
                 cancer_y_train_l, 
                 method = "glm",
                 family = "binomial",
                 preProc = c("center", "scale"))
```

```{r }
# Predict on test data
glm_predict <- predict(glm_fit, newdata = cancer_x_test)
#glm_predict_prob <- predict(glm_fit, newdata = cancer_x_test, type="prob")

```

convert probability outcome into categorical outcome 
```{r }
y_pred_num <- ifelse(glm_predict > 0.5, "B","M")
```

```{r }
# Print Confusion matrix, Accuarcy, Sensitivity etc 
confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test))

# Add results into clf_results dataframe
x3 <- confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test))[["overall"]]
y3 <- confusionMatrix(as.factor(y_pred_num), as.factor(cancer_y_test))[["byClass"]]

clf_results[nrow(clf_results) + 1,] <-  list(Model = "Logistic Regression", 
                                             Accuracy = round (x3[["Accuracy"]],3), 
                                            Precision = round (y3[["Precision"]],3), 
                                            Recall = round (y3[["Recall"]],3), 
                                            F1 = round (y3[["F1"]],3))

# Print Accuracy and F1 score
cat("Accuarcy is ", round(x3[["Accuracy"]],3), "and F1 is ", round (y3[["F1"]],3)  )


```


**Compare Accuracy for all Classification models **

```{r }

print(clf_results)

# Plot accuracy for all the Classification Models

ggplot(clf_results %>% arrange(desc(Accuracy)) %>%
       mutate(Model=factor(Model, levels=Model) ), 
       aes(x = Model, y = Accuracy)) +
  geom_bar(stat = "identity" , width=0.3, fill="steelblue") + 
  coord_cartesian(ylim = c(0.88, 1)) +
  geom_hline(aes(yintercept = mean(Accuracy)),
             colour = "green",linetype="dashed") +
  ggtitle("Compare Accuracy for all Models") +
  theme(plot.title = element_text(color="black", size=10, hjust = 0.5))


```
